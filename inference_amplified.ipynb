{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "from mllib.src.train import main\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.cuda\n",
    "from mllib.src.evaluate import evaluate\n",
    "from mllib.src.utils import prepare_device, load_yaml\n",
    "from mllib.src.distrib import get_dev_wav_clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = \"./mllib/result/mel-rnn/20230202-145405/config.yaml\"\n",
    "# config = './mllib/result/mel-rnn/20230203-121042/config.yaml'\n",
    "\n",
    "# config = \"./mllib/result/dnn/20230202-142249/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-163959/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-170504/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-171624/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-185453/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230203-115011/config.yaml\"\n",
    "\n",
    "# config= \"./mllib/result/unet/20230203-183804/config.yaml\"\n",
    "\n",
    "# config= \"./mllib/result/conv-tasnet/20230203-183838/config.yaml\"\n",
    "\n",
    "# config = \"./result/demucs/20230201-104202/config.yaml\"\n",
    "# config = \"./result/wav-unet/20230201-104328/config.yaml\"\n",
    "# config = \"./result/dcunet/20230201-104116/config.yaml\"\n",
    "\n",
    "# config= \"./result/conv-tasnet/20230207-080249/config.yaml\"\n",
    "# config= \"./result/conv-tasnet/20230207-184607/config.yaml\"    # samples only including target's period, PIT\n",
    "config= \"./result/conv-tasnet/20230207-185011/config.yaml\"    # including all samples, PIT\n",
    "# config= \"./result/conv-tasnet/20230208-175200/config.yaml\"    # including all samples and no PIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = main(path_config=config, return_solver=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = solver.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_yaml(config)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = prepare_device(n_gpu, cudnn_deterministic=args.solver.cudnn_deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = get_dev_wav_clarity(args.dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.src.distrib import get_train_wav_dataset\n",
    "\n",
    "SNR = '0' # '0', '5', '10', '15' # SNR = P_{Signal} / P_{Noise}\n",
    "if args.dset.name == \"Clarity\":\n",
    "\n",
    "    log_clarity = \"./data/metadata/scenes.dev.snr.json\"\n",
    "    metadata = omegaconf.OmegaConf.load(log_clarity)\n",
    "    print(list(metadata.values())[0], list(metadata.keys())[0])\n",
    "    snr_min = 0\n",
    "    snr_max = 5\n",
    "    for data in tqdm.tqdm(dev_dataset, ncols=120):\n",
    "        mixture, sources, origial_length, name = data\n",
    "        scene_name = name.split(\"_\")[0]\n",
    "        if metadata[scene_name] >= snr_min and metadata[scene_name] < snr_max:\n",
    "            data_test = data\n",
    "            snr = metadata[scene_name]\n",
    "            break\n",
    "    \n",
    "    print(\"Clarity dataset SNR: \", snr)\n",
    "\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture, sources, _, name = dev_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.src.model.types import (MULTI_SPEECH_SEPERATION_MODELS,\n",
    "                MULTI_CHANNEL_SEPERATION_MODELS,\n",
    "                MONARCH_SPEECH_SEPARTAION_MODELS, \n",
    "                STFT_MODELS,\n",
    "                WAV_MODELS,)\n",
    "\n",
    "nchannel, nsample = mixture.shape\n",
    "num_spk = sources.shape[1]\n",
    "\n",
    "# mono channel to stereo for source separation models\n",
    "assert args.model.audio_channels == nchannel, f\"Channel between {args.dset.name} and {args.model.name} did not match...\"\n",
    "assert args.model.num_spk == num_spk, f\"number of speakers between {args.dset.name} and {args.model.name} did not match...\"\n",
    "\n",
    "if args.model.name in MULTI_SPEECH_SEPERATION_MODELS:\n",
    "    assert num_spk == len(args.model.sources), f\"number of speakers between {args.dset.name} and {args.model.name} did not match...\"\n",
    "\n",
    "# if not source separation models, merge batch and channels\n",
    "if args.model.name in MONARCH_SPEECH_SEPARTAION_MODELS:\n",
    "    mixture = torch.reshape(mixture, shape=(nchannel, 1, nsample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture.shape, args.model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced = evaluate(mixture=mixture[None], model=model, device=device, config=args)\n",
    "enhanced = torch.squeeze(enhanced, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced.shape, sources.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced = enhanced.detach().cpu()\n",
    "sources = sources.detach().cpu()\n",
    "\n",
    "if args.model.name in MULTI_SPEECH_SEPERATION_MODELS:\n",
    "    enhanced = enhanced[:, 0, ...]\n",
    "    sources = sources[:, 0, ...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced.shape, sources.shape, mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import julius\n",
    "from omegaconf import OmegaConf\n",
    "from recipes.icassp_2023.MLbaseline.enhance  import enhance\n",
    "from recipes.icassp_2023.MLbaseline.evaluate import get_amplified_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_scene = name.split(\"_\")[0]\n",
    "config_clarity_challenge = OmegaConf.load(\"./recipes/icassp_2023/MLbaseline/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_signal_resample = julius.resample.resample_frac(enhanced, args.dset.sample_rate, config_clarity_challenge.nalr.fs)\n",
    "\n",
    "amplified, ref, haspi_score, hasqi_score, audiogram = get_amplified_signal(enhance_signal = enhanced_signal_resample,\n",
    "                                                                fs_signal=config_clarity_challenge.nalr.fs,\n",
    "                                                                scene=name_scene,\n",
    "                                                                cfg=config_clarity_challenge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haspi_score, hasqi_score, audiogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_signal_resample = julius.resample.resample_frac(sources, args.dset.sample_rate, config_clarity_challenge.nalr.fs)\n",
    "\n",
    "amplified_clean, ref_clean, haspi_score_clean, hasqi_score_clean, audiogram = get_amplified_signal(enhance_signal = sources_signal_resample,\n",
    "                                                                fs_signal=config_clarity_challenge.nalr.fs,\n",
    "                                                                scene=name_scene,\n",
    "                                                                cfg=config_clarity_challenge,\n",
    "                                                                audiogram=audiogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haspi_score_clean, hasqi_score_clean, audiogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplified.shape, amplified_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_np = enhanced\n",
    "mixture_np = mixture\n",
    "sources_np = sources\n",
    "amplified_np = amplified\n",
    "amplified_clean_np = amplified_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_np = enhanced_np.numpy().flatten()\n",
    "mixture_np = mixture_np.numpy().flatten()\n",
    "sources_np = sources_np.numpy().flatten()\n",
    "amplified_np = amplified_np.flatten()\n",
    "amplified_clean_np = amplified_clean_np.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_np.shape, mixture_np.shape, sources_np.shape, amplified_np.shape, amplified_clean_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(nrows=6)\n",
    "\n",
    "ax0.plot(mixture_np)\n",
    "ax1.plot(sources_np)\n",
    "ax2.plot(enhanced_np)\n",
    "ax3.plot(amplified_np)\n",
    "ax4.plot(amplified_clean_np)\n",
    "ax5.plot(ref.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(nrows=6, sharey=True)\n",
    "\n",
    "def show_stft(y, _fig, _ax):\n",
    "    D = librosa.stft(y, n_fft=4096)  # STFT of y\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, ax=_ax)\n",
    "    _fig.colorbar(img, ax=_ax)\n",
    "\n",
    "show_stft(mixture_np, fig, ax0)\n",
    "show_stft(sources_np, fig, ax1)\n",
    "show_stft(enhanced_np, fig, ax2)\n",
    "show_stft(amplified_np, fig, ax3)\n",
    "show_stft(amplified_clean_np, fig, ax4)\n",
    "show_stft(ref.flatten(), fig, ax5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(mixture_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(sources_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(enhanced_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(amplified_np, rate=config_clarity_challenge.nalr.fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(amplified_clean_np, rate=config_clarity_challenge.nalr.fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(ref.flatten(), rate=config_clarity_challenge.nalr.fs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_171_daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c9cb1bcb746fd43c70a7667c29c966ffe14df3c625afb960b30bce79fd091af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import torch\n",
    "import julius\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "from mllib.src.train import main\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.cuda\n",
    "from mllib.src.evaluate import evaluate\n",
    "from mllib.src.utils import prepare_device, load_yaml\n",
    "from mllib.src.distrib import get_dev_wav_clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of clarity_challenge and evaluation dataset\n",
    "root_clarity_challenge = '' # [TODO]\n",
    "root_evaluation = \"\" # [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = \"./mllib/result/mel-rnn/20230202-145405/config.yaml\"\n",
    "# config = './mllib/result/mel-rnn/20230203-121042/config.yaml'\n",
    "\n",
    "# config = \"./mllib/result/dnn/20230202-142249/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-163959/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-170504/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-171624/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230202-185453/config.yaml\"\n",
    "# config = \"./mllib/result/dnn/20230203-115011/config.yaml\"\n",
    "\n",
    "# config= \"./mllib/result/unet/20230203-183804/config.yaml\"\n",
    "\n",
    "# config= \"./mllib/result/conv-tasnet/20230203-183838/config.yaml\"\n",
    "\n",
    "# config = \"./result/demucs/20230201-104202/config.yaml\"\n",
    "# config = \"./result/wav-unet/20230201-104328/config.yaml\"\n",
    "# config = \"./result/dcunet/20230201-104116/config.yaml\"\n",
    "\n",
    "# config= \"./result/conv-tasnet/20230207-080249/config.yaml\"\n",
    "# config= \"./result/20230209/conv-tasnet/20230207-184607/config.yaml\"    # samples only including target's period, PIT\n",
    "# config= \"./result/20230209/conv-tasnet/20230207-185011/config.yaml\"    # including all samples, PIT\n",
    "# config= \"./result/20230209/conv-tasnet/20230208-175200/config.yaml\"    # including all samples and no PIT\n",
    "\n",
    "# config = \"./result/conv-tasnet/20230215-115103/config.yaml\" \n",
    "config = \"./result/conv-tasnet/20230216-113419/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = main(obj_config=config, return_solver=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = solver.model\n",
    "del solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_yaml(config)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = prepare_device(n_gpu, cudnn_deterministic=args.solver.cudnn_deterministic)\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# Evaluation \n",
    "# Best, haspi, hasqi\n",
    "# ('S11393', 'L0146', 'S10435', 'L0109')\n",
    "# Worst, haspi, hasqi\n",
    "# ('S10841', 'L0146', 'S10934', 'L0138')\n",
    "\n",
    "# scene_name= 'S10841'\n",
    "# listener = 'L0146'\n",
    "\n",
    "scene_name= 'S11393'\n",
    "listener = 'L0146'\n",
    "\n",
    "eval1 = root_evaluation + \"/eval1\"\n",
    "\n",
    "target_channel = 1\n",
    "wavfile = eval1 + f\"/scenes/{scene_name}_mix_CH{target_channel}.wav\"\n",
    "wav, sr = sf.read(wavfile)\n",
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture = np.transpose(wav, (1, 0))\n",
    "mixture = torch.tensor(data=mixture, dtype=torch.float32, device=device)\n",
    "mixture = julius.resample_frac(mixture, sr, args.dset.sample_rate)\n",
    "mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(mixture.flatten().numpy(), rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.src.model.types import (MULTI_SPEECH_SEPERATION_MODELS,\n",
    "                MULTI_CHANNEL_SEPERATION_MODELS,\n",
    "                MONARCH_SPEECH_SEPARTAION_MODELS, \n",
    "                STFT_MODELS,\n",
    "                WAV_MODELS,)\n",
    "\n",
    "nchannel, nsample = mixture.shape\n",
    "# num_spk = sources.shape[1]\n",
    "\n",
    "# mono channel to stereo for source separation models\n",
    "assert args.model.audio_channels == nchannel, f\"Channel between {args.dset.name} and {args.model.name} did not match...\"\n",
    "# assert args.model.num_spk == num_spk, f\"number of speakers between {args.dset.name} and {args.model.name} did not match...\"\n",
    "\n",
    "# if args.model.name in MULTI_SPEECH_SEPERATION_MODELS:\n",
    "#     assert num_spk == len(args.model.sources), f\"number of speakers between {args.dset.name} and {args.model.name} did not match...\"\n",
    "\n",
    "# if not source separation models, merge batch and channels\n",
    "if args.model.name in MONARCH_SPEECH_SEPARTAION_MODELS:\n",
    "    mixture = torch.reshape(mixture, shape=(nchannel, 1, nsample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture.shape, args.model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced = evaluate(mixture=mixture[None], model=model, device=device, config=args)\n",
    "enhanced = torch.squeeze(enhanced, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced.shape, # sources.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced = enhanced.detach().cpu()\n",
    "\n",
    "if args.model.name in MULTI_SPEECH_SEPERATION_MODELS:\n",
    "    enhanced = enhanced[0, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced.shape, mixture.shape, type(enhanced), type(mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from clarity.enhancer.compressor import Compressor\n",
    "from clarity.enhancer.nalr import NALR\n",
    "from recipes.icassp_2023.MLbaseline.enhance  import enhance\n",
    "from recipes.icassp_2023.MLbaseline.evaluate import get_amplified_signal, amplify_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "# Best, haspi, hasqi\n",
    "# ('S11393', 'L0146', 'S10435', 'L0109')\n",
    "# Worst, haspi, hasqi\n",
    "# ('S10841', 'L0146', 'S10934', 'L0138')\n",
    "config_clarity_challenge = OmegaConf.load(\"./recipes/icassp_2023/MLbaseline/config_eval.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeners_file = root_clarity_challenge+ 'clarity_CEC2_icassp2023_eval/clarity_data/metadata/listeners.eval.json'\n",
    "scene_file = root_clarity_challenge+ 'clarity_CEC2_icassp2023_eval/clarity_data/metadata/listeners.eval.json'\n",
    "\n",
    "with open(listeners_file, \"r\", encoding=\"utf-8\") as fp:\n",
    "    listener_audiograms = json.load(fp)\n",
    "audiogram = listener_audiograms[listener]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeners_file = root_evaluation + '/metadata/listeners.eval.json'\n",
    "\n",
    "\n",
    "with open(listeners_file, \"r\", encoding=\"utf-8\") as fp:\n",
    "    listener_audiograms = json.load(fp)\n",
    "audiogram = listener_audiograms[listener]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_resample = julius.resample.resample_frac(enhanced, args.dset.sample_rate, config_clarity_challenge.nalr.fs)\n",
    "mixture_resample = julius.resample.resample_frac(mixture, args.dset.sample_rate, config_clarity_challenge.nalr.fs)\n",
    "\n",
    "enhancer = NALR(**config_clarity_challenge.nalr)\n",
    "compressor = Compressor(**config_clarity_challenge.compressor)\n",
    "\n",
    "out_l = amplify_signal(enhanced_resample[0, ...], audiogram, \"l\", enhancer, compressor)\n",
    "out_r = amplify_signal(enhanced_resample[1, ...], audiogram, \"r\", enhancer, compressor)\n",
    "amplified = np.stack([out_l, out_r], axis=0)\n",
    "\n",
    "if config_clarity_challenge.soft_clip:\n",
    "    amplified = np.tanh(amplified)\n",
    "\n",
    "amplified_sample_model = julius.resample.resample_frac(torch.tensor(amplified, dtype=torch.float32), \n",
    "                                                        config_clarity_challenge.nalr.fs, \n",
    "                                                        args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiogram, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_np = enhanced\n",
    "mixture_np = mixture\n",
    "amplified_np = amplified_sample_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(amplified_np), type(enhanced_np), type(mixture_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_np = enhanced_np.numpy().flatten()\n",
    "mixture_np = mixture_np.numpy().flatten()\n",
    "amplified_np = amplified_np.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_np.shape, mixture_np.shape, amplified_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=3)\n",
    "\n",
    "ax0.plot(mixture_np)\n",
    "ax1.plot(enhanced_np)\n",
    "ax2.plot(amplified_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, sharey=True)\n",
    "\n",
    "def show_stft(y, _fig, _ax):\n",
    "    D = librosa.stft(y, n_fft=4096)  # STFT of y\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, ax=_ax)\n",
    "    _fig.colorbar(img, ax=_ax)\n",
    "\n",
    "show_stft(mixture_np, fig, ax0)\n",
    "show_stft(enhanced_np, fig, ax1)\n",
    "show_stft(amplified_np, fig, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(mixture_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(enhanced_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(amplified_np, rate=args.dset.sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_171_daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c9cb1bcb746fd43c70a7667c29c966ffe14df3c625afb960b30bce79fd091af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
